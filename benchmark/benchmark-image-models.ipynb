{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eebee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# A prediction is 'True' (Violent) if confidence > THRESHOLD.\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "TFLITE_OUTPUT_DIR = \"/home/kronbii/repos/content-violence-detection/output/TFLite\"\n",
    "KERAS_OUTPUT_DIR = \"/home/kronbii/repos/content-violence-detection/output/keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d49b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance(base_dir):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics for a model based on a confidence threshold.\n",
    "    The directory should contain 'V' (Violent) and 'NV' (Non-Violent) subdirectories.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # --- Process Violent (V) Category ---\n",
    "    v_path = os.path.join(base_dir, 'V')\n",
    "    if not os.path.exists(v_path):\n",
    "        print(f\"Warning: Directory not found: {v_path}\")\n",
    "        return None\n",
    "\n",
    "    v_files = glob.glob(os.path.join(v_path, '*.csv'))\n",
    "    total_v_frames = 0\n",
    "    true_positives_v = 0\n",
    "\n",
    "    for f in v_files:\n",
    "        df = pd.read_csv(f)\n",
    "        total_v_frames += len(df)\n",
    "        # A True Positive is when confidence < THRESHOLD for a violent video.\n",
    "        true_positives_v += (df['confidence'] >= CONFIDENCE_THRESHOLD).sum()\n",
    "\n",
    "    results['total_v_frames'] = total_v_frames\n",
    "    results['true_positives_v'] = true_positives_v\n",
    "    results['accuracy_v'] = (true_positives_v / total_v_frames * 100) if total_v_frames > 0 else 0\n",
    "\n",
    "    # --- Process Non-Violent (NV) Category ---\n",
    "    nv_path = os.path.join(base_dir, 'NV')\n",
    "    if not os.path.exists(nv_path):\n",
    "        print(f\"Warning: Directory not found: {nv_path}\")\n",
    "        return None\n",
    "\n",
    "    nv_files = glob.glob(os.path.join(nv_path, '*.csv'))\n",
    "    total_nv_frames = 0\n",
    "    correct_predictions_nv = 0\n",
    "\n",
    "    for f in nv_files:\n",
    "        df = pd.read_csv(f)\n",
    "        total_nv_frames += len(df)\n",
    "        # A correct prediction is when confidence >= THRESHOLD for a non-violent video.\n",
    "        correct_predictions_nv += (df['confidence'] < CONFIDENCE_THRESHOLD).sum()\n",
    "\n",
    "    results['total_nv_frames'] = total_nv_frames\n",
    "    results['correct_predictions_nv'] = correct_predictions_nv\n",
    "    results['accuracy_nv'] = (correct_predictions_nv / total_nv_frames * 100) if total_nv_frames > 0 else 0\n",
    "\n",
    "    # --- Calculate Total Accuracy ---\n",
    "    total_correct = true_positives_v + correct_predictions_nv\n",
    "    total_frames = total_v_frames + total_nv_frames\n",
    "    results['total_frames'] = total_frames\n",
    "    results['total_accuracy'] = (total_correct / total_frames * 100) if total_frames > 0 else 0\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_agreement(dir1, dir2):\n",
    "    \"\"\"\n",
    "    Calculates the frame-by-frame prediction agreement between two models\n",
    "    based on the confidence threshold.\n",
    "\n",
    "    This version assumes that for every CSV file in a category in dir1,\n",
    "    an identical one exists in dir2.\n",
    "    \"\"\"\n",
    "    total_frames_compared = 0\n",
    "    agreements = 0\n",
    "\n",
    "    for category in ['V', 'NV']:\n",
    "        path1 = os.path.join(dir1, category)\n",
    "        path2 = os.path.join(dir2, category)\n",
    "\n",
    "        # Skip if the category directory doesn't exist in the first model's output\n",
    "        if not os.path.exists(path1):\n",
    "            print(f\"Warning: Directory {path1} not found. Skipping agreement for '{category}'.\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through files in the first directory, assuming they exist in the second\n",
    "        for file_path1 in glob.glob(os.path.join(path1, '*.csv')):\n",
    "            filename = os.path.basename(file_path1)\n",
    "            file_path2 = os.path.join(path2, filename)\n",
    "\n",
    "            # Read the corresponding files\n",
    "            df1 = pd.read_csv(file_path1)\n",
    "            df2 = pd.read_csv(file_path2)\n",
    "\n",
    "            # A check for matching frame counts is still a good safety measure\n",
    "            if len(df1) != len(df2):\n",
    "                print(f\"Warning: Frame count mismatch for {filename}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "            # Determine predictions based on the threshold for each model\n",
    "            preds1 = df1['confidence'] >= CONFIDENCE_THRESHOLD\n",
    "            preds2 = df2['confidence'] >= CONFIDENCE_THRESHOLD\n",
    "\n",
    "            total_frames_compared += len(df1)\n",
    "            # Count where both predictions are the same (both True or both False)\n",
    "            agreements += (preds1 == preds2).sum()\n",
    "\n",
    "    if total_frames_compared == 0:\n",
    "        return 0\n",
    "\n",
    "    return (agreements / total_frames_compared * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4843ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model_name, results):\n",
    "    \"\"\"Helper function to neatly print the analysis results.\"\"\"\n",
    "    if results is None:\n",
    "        print(f\"Could not generate results for {model_name}.\\n\")\n",
    "        return\n",
    "\n",
    "    print(f\"Results for {model_name} (Threshold: {CONFIDENCE_THRESHOLD}):\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"Violent Videos (V):\")\n",
    "    print(f\"  - Total Frames: {results['total_v_frames']:,}\")\n",
    "    print(f\"  - Correctly Identified (TP): {results['true_positives_v']:,}\")\n",
    "    print(f\"  - Accuracy: {results['accuracy_v']:.2f}%\")\n",
    "\n",
    "    print(f\"\\nNon-Violent Videos (NV):\")\n",
    "    print(f\"  - Total Frames: {results['total_nv_frames']:,}\")\n",
    "    print(f\"  - Correctly Identified (TN): {results['correct_predictions_nv']:,}\")\n",
    "    print(f\"  - Accuracy: {results['accuracy_nv']:.2f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 45)\n",
    "    print(f\"Total Accuracy: {results['total_accuracy']:.2f}% on {results['total_frames']:,} frames.\")\n",
    "    print(\"-\" * 45 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15787da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze TFLite Model ---\n",
    "tflite_results = analyze_model_performance(TFLITE_OUTPUT_DIR)\n",
    "print_results(\"TFLite Model\", tflite_results)\n",
    "\n",
    "# --- Analyze Keras Model ---\n",
    "keras_results = analyze_model_performance(KERAS_OUTPUT_DIR)\n",
    "print_results(\"Keras Model\", keras_results)\n",
    "\n",
    "# --- Compare Model Agreement ---\n",
    "if tflite_results and keras_results:\n",
    "    agreement_percentage = calculate_agreement(TFLITE_OUTPUT_DIR, KERAS_OUTPUT_DIR)\n",
    "    print(\"Model Agreement:\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"The two models agree on the prediction for {agreement_percentage:.2f}% of all frames.\")\n",
    "    print(\"-\" * 45)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
