{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aff5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef45ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input text and ground truth\n",
    "input_df = pd.read_csv(\"/home/kronbii/repos/content-violence-detection/datasets/text/jigsaw/test.csv\")  # contains 'id' and 'text'\n",
    "gt_df = pd.read_csv(\"/home/kronbii/repos/content-violence-detection/datasets/text/jigsaw/test_labels.csv\")   # contains 'id' and 6 labels\n",
    "\n",
    "# Merge on ID to ensure alignment\n",
    "data = input_df.merge(gt_df, on=\"id\", how=\"inner\")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label columns to check for -1\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Remove all rows where any label is -1\n",
    "clean_data = data[~(data[label_cols] == -1).any(axis=1)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Rows before cleaning: {len(data)}\")\n",
    "print(f\"Rows after removing -1 labels: {len(clean_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f525cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions\n",
    "all_preds = []\n",
    "\n",
    "batch_size = 32\n",
    "texts = clean_data['comment_text'].tolist()\n",
    "ids = clean_data['id'].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size)):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encodings).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_preds.append(probs)\n",
    "\n",
    "# Combine predictions into final array\n",
    "all_preds = np.vstack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build output DataFrame\n",
    "pred_df = pd.DataFrame(all_preds, columns=label_cols)\n",
    "pred_df.insert(0, 'id', ids)\n",
    "\n",
    "# Save to CSV\n",
    "pred_df.to_csv(\"predictions.csv\", index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6222ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -1 in ground truth (assumes -1 means positive label)\n",
    "print(clean_data.columns.tolist())\n",
    "\n",
    "y_true = clean_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].replace(-1, 1).values\n",
    "y_pred_probs = pred_df[label_cols].values\n",
    "\n",
    "# Binarize predictions\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "# Per-label metrics\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(\"Per-label Evaluation:\\n\")\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "    true = y_true[:, i]\n",
    "    pred = y_pred[:, i]\n",
    "\n",
    "    total = len(true)\n",
    "    tp = ((true == 1) & (pred == 1)).sum()\n",
    "    tn = ((true == 0) & (pred == 0)).sum()\n",
    "    acc = accuracy_score(true, pred)\n",
    "    f1 = f1_score(true, pred, average='binary', zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  Total samples:     {total}\")\n",
    "    print(f\"  Matches dets:      {tp + tn}\")\n",
    "    print(f\"  Accuracy:          {acc:.4f}\")\n",
    "    print(f\"  F1 Score:          {f1:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
